---
sudo: required
dist: bionic
env:
  global:
  - KUBECTL_VERSION=1.18.3
  - KUBERNETES_VERSION=1.18.3
  - KUBECONFIG=$HOME/.kube/config

notifications:
  slack:
    on_failure: always

#https://istio.io/docs/setup/platform-setup/gardener/
#https://github.com/gardener/gardener/blob/master/docs/development/local_setup.md
fleet_script_gardener_macos_tasks : &fleet_script_gardener_macos_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
        # Install kind via brew
        - /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"  #Install brew
        - brew install kubernetes-cli #Installing kubectl and helm
        - brew install kubernetes-helm
        - brew install git #Installing git
        - brew install openvpn #Installing openvpn
        - export PATH=$(brew --prefix openvpn)/sbin:$PATH
        #Alternatively, you can also install Docker for Desktop and kind.
        #Installing Minikube
        - brew install minikube #https://minikube.sigs.k8s.io/docs/start/
        - which minikube
        #Alternatively,Installing Minikube
        # - curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64
        # - sudo install minikube-darwin-amd64 /usr/local/bin/minikube
        # - which minikube
        # - brew cask remove minikube #If which minikube fails after installation via brew, you may have to remove the minikube cask and link the binary
        # - brew link minikube
        - brew install iproute2mac #Installing iproute2
        - go get -u github.com/bronze1man/yaml2json #Installing yaml2json and jq
        - brew install jq
        # - brew install coreutils gnu-sed #Install GNU core utilities,Error: coreutils 8.31 is already installed
        #Local Gardener setup
        - git clone git@github.com:gardener/gardener.git && cd gardener
        # Using the nodeless cluster setup,Setting up a local nodeless Garden cluster is quite simple
        # The only prerequisite is a running docker daemon. Just use the provided Makefile rules to start your local Garden
        - make local-garden-up #start all minimally required components of a Kubernetes cluster (etcd, kube-apiserver, kube-controller-manager) and an etcd Instance for the gardener-apiserver as Docker containers
        - make local-garden-down #tear down the local Garden cluster and remove the Docker containers
        # istio Kubernetes Gardener Bootstrapping Gardener #https://istio.io/docs/setup/platform-setup/gardener/
        #Install and configure kubectl https://kubernetes.io/docs/tasks/tools/install-kubectl/
        - curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl"
        - chmod +x ./kubectl #Make the kubectl binary executable
        - sudo mv ./kubectl /usr/local/bin/kubectl #Move the binary in to your PATH
        - kubectl version --client #Test to ensure the version you installed is up-to-date

fleet_script_kind_istio_tasks : &fleet_script_kind_istio_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - docker version
          - curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64
          - chmod +x ./kind
          - sudo mv ./kind /usr/local/bin/kind
          - kind get clusters #see the list of kind clusters
          - kind create cluster --name istio-testing #Create a cluster,By default, the cluster will be given the name kind
          - kind get clusters
          # - sudo snap install kubectl --classic
          - kubectl config get-contexts #list the local Kubernetes contexts
          - kubectl config use-context kind-istio-testing #run following command to set the current context for kubectl
          - kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml #deploy Dashboard
          - echo "===============================Waiting for Dashboard to be ready==========================================================="
          - kubectl get service --all-namespaces #list all services in all namespace
          - |
            for i in {1..60}; do # Timeout after 5 minutes, 60x2=120 secs, 2 mins
              if kubectl get pods --namespace=kubernetes-dashboard |grep Running && \
                 kubectl get pods --namespace=dashboard-metrics-scraper |grep Running ; then
                break
              fi
              sleep 2
            done
          - kubectl get pod -n kubernetes-dashboard #Verify that Dashboard is deployed and running
          - kubectl create clusterrolebinding default-admin --clusterrole cluster-admin --serviceaccount=default:default #Create a ClusterRoleBinding to provide admin access to the newly created cluster
          #To login to Dashboard, you need a Bearer Token. Use the following command to store the token in a variable
          - token=$(kubectl get secrets -o jsonpath="{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='default')].data.token}"|base64 --decode)
          - echo $token #Display the token using the echo command and copy it to use for logging into Dashboard.
          - kubectl proxy & # Access Dashboard using the kubectl command-line tool by running the following command, Starting to serve on 127.0.0.1:8001
          - echo "===============================Install istio==========================================================="
          - 'curl -L https://istio.io/downloadIstio | sh -' #Download Istio
          -  cd istio-* #Move to the Istio package directory. For example, if the package is istio-1.6.0
          - export PATH=$PWD/bin:$PATH #Add the istioctl client to your path, The istioctl client binary in the bin/ directory.
          #precheck inspects a Kubernetes cluster for Istio install requirements
          - istioctl experimental precheck #https://istio.io/docs/reference/commands/istioctl/#istioctl-experimental-precheck
          #Begin the Istio pre-installation verification check
          # - istioctl verify-install #Error: could not load IstioOperator from cluster: the server could not find the requested resource.  Use --filename
          - istioctl version
          - istioctl manifest apply --set profile=demo #Install Istio, use the demo configuration profile
          - kubectl label namespace default istio-injection=enabled #Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later
          - kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml #Deploy the Bookinfo sample application:
          - kubectl get service --all-namespaces #list all services in all namespace
          - kubectl get services #The application will start. As each pod becomes ready, the Istio sidecar will deploy along with it.
          - kubectl get pods
          - |
            for i in {1..60}; do # Timeout after 5 minutes, 60x2=120 secs, 2 mins
              if kubectl get pods --namespace=istio-system |grep Running ; then
                break
              fi
              sleep 2
            done
          - kubectl get service --all-namespaces #list all services in all namespace
          # - |
          #   kubectl exec -it $(kubectl get pod \
          #                -l app=ratings \
          #                -o jsonpath='{.items[0].metadata.name}') \
          #                -c ratings \
          #                -- curl productpage:9080/productpage | grep -o "<title>.*</title>" <title>Simple Bookstore App</title>
          #Open the application to outside traffic
          #The Bookinfo application is deployed but not accessible from the outside. To make it accessible, you need to create an Istio Ingress Gateway, which maps a path to a route at the edge of your mesh.
          # - kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml #Associate this application with the Istio gateway
          # - istioctl analyze #Ensure that there are no issues with the configuration
          #Determining the ingress IP and ports
          #If the EXTERNAL-IP value is set, your environment has an external load balancer that you can use for the ingress gateway.
          # - kubectl get svc istio-ingressgateway -n istio-system #determine if your Kubernetes cluster is running in an environment that supports external load balancers
          # #Follow these instructions if you have determined that your environment has an external load balancer.
          # # If the EXTERNAL-IP value is <none> (or perpetually <pending>), your environment does not provide an external load balancer for the ingress gateway,access the gateway using the service’s node port.
          # - export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
          # - export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
          # - export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')
          # #In certain environments, the load balancer may be exposed using a host name, instead of an IP address.
          # #the ingress gateway’s EXTERNAL-IP value will not be an IP address, but rather a host name
          # #failed to set the INGRESS_HOST environment variable, correct the INGRESS_HOST value
          # - export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          # #Follow these instructions if your environment does not have an external load balancer and choose a node port instead
          # - export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}') #Set the ingress ports
          # - export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}') #Set the ingress ports
          # - export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT #Set GATEWAY_URL
          # - echo $GATEWAY_URL #Ensure an IP address and port were successfully assigned to the environment variable
          # # - echo http://$GATEWAY_URL/productpage #Verify external access,retrieve the external address of the Bookinfo application
          # - istioctl dashboard kiali #optional dashboards installed by the demo installation,Access the Kiali dashboard. The default user name is admin and default password is admin
          # #The Istio uninstall deletes the RBAC permissions and all resources hierarchically under the istio-system namespace
          # #It is safe to ignore errors for non-existent resources because they may have been deleted hierarchically.
          # - 'istioctl manifest generate --set profile=demo | kubectl delete -f -'
          # - kubectl delete namespace istio-system #The istio-system namespace is not removed by default. If no longer needed, use the following command to remove it
          # - kubectl get virtualservices   #-- there should be no virtual services
          # - kubectl get destinationrules  #-- there should be no destination rules
          # - kubectl get gateway           #-- there should be no gateway
          # - kubectl get pods              #-- the Bookinfo pods should be deleted
          # #Bookinfo cleanup starts
          # - |
          #   SCRIPTDIR=$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )
          #   # only ask if in interactive mode
          #   if [[ -t 0 && -z ${NAMESPACE} ]];then
          #     echo -n "namespace ? [default] "
          #     read -r NAMESPACE
          #   fi
          #   if [[ -z ${NAMESPACE} ]];then
          #     NAMESPACE=default
          #   fi
          #   echo "using NAMESPACE=${NAMESPACE}"
          #   protos=( destinationrules virtualservices gateways )
          #   for proto in "${protos[@]}"; do
          #     for resource in $(kubectl get -n ${NAMESPACE} "$proto" -o name); do
          #       kubectl delete -n ${NAMESPACE} "$resource";
          #     done
          #   done
          #   OUTPUT=$(mktemp)
          #   export OUTPUT
          #   echo "Application cleanup may take up to one minute"
          #   kubectl delete -n ${NAMESPACE} -f "$SCRIPTDIR/bookinfo.yaml" > "${OUTPUT}" 2>&1
          #   ret=$?
          #   function cleanup() {
          #     rm -f "${OUTPUT}"
          #   }
          #   trap cleanup EXIT
          #   if [[ ${ret} -eq 0 ]];then
          #     cat "${OUTPUT}"
          #   else
          #     # ignore NotFound errors
          #     OUT2=$(grep -v NotFound "${OUTPUT}")
          #     if [[ -n ${OUT2} ]];then
          #       cat "${OUTPUT}"
          #       exit ${ret}
          #     fi
          #   fi
          #   echo "Application cleanup successful"
          # - kubectl get virtualservices   #-- there should be no virtual services
          # - kubectl get destinationrules  #-- there should be no destination rules
          # - kubectl get gateway           #-- there should be no gateway
          # - kubectl get pods              #-- the Bookinfo pods should be deleted
          #Bookinfo cleanup ends
          # - echo "===============================Adding Heapster Metrics to the Kubernetes Dashboard==========================================================="
          # - sudo snap install helm --classic && helm init
          # - kubectl create serviceaccount --namespace kube-system tiller #Create a service account
          # - kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller #Bind the new service account to the cluster-admin role. This will give tiller admin access to the entire cluster
          # - kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}' #Deploy tiller and add the line serviceAccount: tiller to spec.template.spec
          # - helm install --name heapster stable/heapster --namespace kube-system #install Heapster
          - kind delete cluster --name istio-testing #delete the existing cluster

fleet_script_dashboard_tasks : &fleet_script_dashboard_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - docker version
          - curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64
          - chmod +x ./kind
          - sudo mv ./kind /usr/local/bin/kind
          - kind get clusters #see the list of kind clusters
          - kind create cluster --name istio-testing #Create a cluster,By default, the cluster will be given the name kind
          - kind get clusters
          - sudo snap install kubectl --classic
          - kubectl config get-contexts #list the local Kubernetes contexts
          - kubectl config use-context kind-istio-testing #run following command to set the current context for kubectl
          - kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml #deploy Dashboard
          - echo "===============================Waiting for Dashboard to be ready==========================================================="
          - kubectl get service --all-namespaces #list all services in all namespace
          - |
            for i in {1..60}; do # Timeout after 5 minutes, 60x2=120 secs, 2 mins
              if kubectl get pods --namespace=kubernetes-dashboard |grep Running && \
                 kubectl get pods --namespace=dashboard-metrics-scraper |grep Running ; then
                break
              fi
              sleep 2
            done
          - kubectl get pod -n kubernetes-dashboard #Verify that Dashboard is deployed and running
          - kubectl create clusterrolebinding default-admin --clusterrole cluster-admin --serviceaccount=default:default #Create a ClusterRoleBinding to provide admin access to the newly created cluster
          #To login to Dashboard, you need a Bearer Token. Use the following command to store the token in a variable
          - token=$(kubectl get secrets -o jsonpath="{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='default')].data.token}"|base64 --decode)
          - echo $token #Display the token using the echo command and copy it to use for logging into Dashboard.
          - kubectl proxy & # Access Dashboard using the kubectl command-line tool by running the following command, Starting to serve on 127.0.0.1:8001

fleet_script_tasks : &fleet_script_tasks
      script:
        - python --version
fleet_install_tasks : &fleet_install_tasks
      install:
        - pip install -r requirements.txt


matrix:
  fast_finish: true
  include:

    - name: "openesb kind Python 3.7 on bionic" #OK
      dist: bionic
      arch: amd64
      addons:
        snaps:
          - name: kubectl
            confinement: classic # or devmode
            channel: latest/stable # will be passed to --channel flag
          - name: helm
            confinement: classic # or devmode
            channel: latest/stable # will be passed to --channel flag
      language: python
      python: 3.7
      before_install:
        - pip3 install virtualenv
        - virtualenv -p $(which python3) ~venvpy3
        - source ~venvpy3/bin/activate
      <<: *fleet_install_tasks
      <<: *fleet_script_tasks
      script:
        - sudo make deploy-kind
        # - sudo kind create cluster --name openesb-testing
        # - sudo kubectl config use-context kind-openesb-testing
        # - sudo make deploy-openesb
        # - sudo kind delete cluster --name openesb-testing
        # - sudo kind create cluster --name dashboard-testing
        # - sudo kubectl config use-context kind-dashboard-testing
        # - sudo make deploy-dashboard
        # - sudo kind delete cluster --name dashboard-testing
        - sudo kind create cluster --name dashboard-testing
        - sudo kubectl config use-context kind-dashboard-testing
        - sudo make deploy-dashboard-helm
        - sudo kind delete cluster --name dashboard-testing
      after_success:
        - deactivate


    # - name: "kind gardener  Python 3.7 on bionic"
    #   dist: bionic
    #   arch: amd64
    #   addons:
    #     snaps:
    #       - name: kubectl
    #         confinement: classic # or devmode
    #         channel: latest/stable # will be passed to --channel flag
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_dashboard_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "kind istio  Python 3.7 on bionic"
    #   dist: bionic
    #   arch: amd64
    #   addons:
    #     snaps:
    #       - name: kubectl
    #         confinement: classic # or devmode
    #         channel: latest/stable # will be passed to --channel flag
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_kind_istio_tasks
    #   after_success:
    #     - deactivate
    #
    #
    # - name: "kind  Python 3.7 on bionic"
    #   dist: bionic
    #   arch: amd64
    #   addons:
    #     snaps:
    #       - name: kubectl
    #         confinement: classic # or devmode
    #         channel: latest/stable # will be passed to --channel flag
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_kind_istio_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "kind gardener  Python 3.7 on xenial"
    #   dist: xenial
    #   arch: amd64
    #   addons:
    #     snaps:
    #       - name: kubectl
    #         confinement: classic # or devmode
    #         channel: latest/stable # will be passed to --channel flag
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_dashboard_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "kind  istio Python 3.7 on xenial"
    #   dist: xenial
    #   arch: amd64
    #   addons:
    #     snaps:
    #       - name: kubectl
    #         confinement: classic # or devmode
    #         channel: latest/stable # will be passed to --channel flag
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_kind_istio_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "kind  Python 3.7 on xenial"
    #   dist: xenial
    #   arch: amd64
    #   addons:
    #     snaps:
    #       - name: kubectl
    #         confinement: classic # or devmode
    #         channel: latest/stable # will be passed to --channel flag
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_dashboard_tasks
    #   after_success:
    #     - deactivate

    # - name: "Python 3.7 on bionic arm64" # package architecture (amd64) does not match system (arm64)
    #   os: linux
    #   arch: arm64
    #   dist: bionic
    #   language: python
    #   python: 3.7
    #   # env:
    #   #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
    #   # compiler:
    #   #  - gcc
    #   #  - clang
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_vagrant_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "Python 3.7 on bionic ppc64le" #Unable to locate package osquery
    #   os: linux
    #   arch: ppc64le
    #   dist: bionic
    #   language: python
    #   python: 3.7
    #   # env:
    #   #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
    #   # compiler:
    #   #  - gcc
    #   #  - clang
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_vagrant_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "Python 3.7 on bionic s390x" #Unable to locate package osquery
    #   os: linux
    #   arch: s390x
    #   dist: bionic
    #   language: python
    #   python: 3.7
    #   # env:
    #   #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
    #   # compiler:
    #   #  - gcc
    #   #  - clang
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_vagrant_tasks
    #   after_success:
    #     - deactivate


    # - name: "Python 2.7 on xenial amd64"
    #   dist: xenial
    #   language: python
    #   python: 2.7
    #   before_install:
    #     - pip install virtualenv
    #     - virtualenv -p $(which python2) ~venvpy2
    #     - source ~venvpy2/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #
    #   after_success:
    #     - deactivate

    # - name: "Python 3.7 on xenial arm64"
    #   os: linux
    #   arch: arm64
    #   dist: xenial
    #   language: python
    #   python: 3.7
    #   # env:
    #   #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
    #   # compiler:
    #   #  - gcc
    #   #  - clang
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_vagrant_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "Python 3.7 on xenial ppc64le" #Unable to locate package osquery
    #   os: linux
    #   arch: ppc64le
    #   dist: xenial
    #   language: python
    #   python: 3.7
    #   # env:
    #   #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
    #   # compiler:
    #   #  - gcc
    #   #  - clang
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_vagrant_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "Python 3.7 on xenial s390x" #Unable to locate package osquery
    #   os: linux
    #   arch: s390x
    #   dist: xenial
    #   language: python
    #   python: 3.7
    #   # env:
    #   #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
    #   # compiler:
    #   #  - gcc
    #   #  - clang
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_vagrant_tasks
    #   after_success:
    #     - deactivate




    # - name: "kind brew Python 2.7.17 on macOS xcode10.2"
    #   os: osx
    #   osx_image: xcode10.2
    #   language: shell
    #   before_install:
    #     - pip install virtualenv
    #     - virtualenv -p $(which python2) ~venvpy2
    #     - source ~venvpy2/bin/activate
    #     # Install kind via brew
    #     - /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"  #Install brew
    #     - brew install kind
        # - kind create cluster # Default cluster context name is `kind`.
        # - kind create cluster --name kind-2
        # - kind get clusters # list kind clusters
        # - kubectl cluster-info --context kind-kind #In order to interact with a specific cluster, you only need to specify the cluster name as a context in kubectl
        # - kind load docker-image hello-world #Docker images can be loaded into your cluster nodes
        # # - kind load image-archive /my-image-archive.tar #image archives can be loaded
        # # custom DockerFile build workflow starts, don't use a :latest tag
        # # - docker build -t my-custom-image:unique-tag ./my-image-dir
        # # - kind load docker-image my-custom-image:unique-tag
        # # - kubectl apply -f my-manifest-using-my-image:unique-tag
        # # - docker exec -it my-node-name crictl images # get a list of images present on a cluster node,my-node-name is the name of the Docker container
        # # - kind build node-image --type bazel #by using docker or bazel. To specify the build type use the flag --type
        # # custom DockerFile build workflow ends
        # - kubectl cluster-info --context kind-kind-2
        # - kind load docker-image hello-world --name kind-2 #If using a named cluster you will need to specify the name of the cluster you wish to load the image into
        # - kind delete cluster #If the flag --name is not specified, kind uses the default cluster context name kind and deletes that cluster
        # - kind delete cluster --name kind-2
        # - kind get clusters # list kind clusters
      # <<: *fleet_install_tasks
      # <<: *fleet_script_tasks
      # after_success:
      #   - deactivate

    # - name: "gardener Python 2.7.17 on macOS xcode10.2"
    #   os: osx
    #   osx_image: xcode10.2
    #   language: shell
    #   before_install:
    #     - pip install virtualenv
    #     - virtualenv -p $(which python2) ~venvpy2
    #     - source ~venvpy2/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_gardener_macos_tasks
    #   after_success:
    #     - deactivate




    # - name: "Python 3.7.5 on macOS xcode10.2"
    #   os: osx
    #   osx_image: xcode10.2
    #   language: shell
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "Python 3.7.5 on macOS xcode9.4 "
    #   os: osx
    #   osx_image: xcode9.4
    #   language: shell
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   after_success:
    #     - deactivate



    # - name: "kind chocolatey Python 3.8 on Windows"
    #   os: windows
    #   language: shell
    #   env:
    #     - PATH=/c/Python38:/c/Python38/Scripts:$PATH
    #   before_install:
    #     - choco install python --version 3.8.1
    #     - pip install virtualenv
    #     - virtualenv $HOME/venv
    #     - source $HOME/venv/Scripts/activate
    #     # Install kind via chocolatey
    #     # - Get-ExecutionPolicy #If it returns Restricted, then run Set-ExecutionPolicy AllSigned or Set-ExecutionPolicy Bypass -Scope Process.
    #     # - Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))
    #     - choco install kind
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   after_success:
    #     - deactivate

    # - name: "Python 3.7 on Windows"
    #   os: windows
    #   language: shell
    #   env: PATH=/c/Python37:/c/Python37/Scripts:$PATH
    #   before_install:
    #     - choco install python --version 3.7.3
    #     - python -m pip install virtualenv
    #     - virtualenv $HOME/venv
    #     - source $HOME/venv/Scripts/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   after_success:
    #     - deactivate
